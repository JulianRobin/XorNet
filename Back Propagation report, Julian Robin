Back Propagation report, Julian Robin

In order to begin discussion surrounding the results of running my neural network, some clarifications must first be discussed. As my implementation initializes the weights between neurons with random values, the results discussed from this point forward may not exactly align with results from another run of the neural network. This is due to the fact that the XOR problem does not require the predictions to be precise enough to a point where the randomness is imperceptible after many iterations of back propagation. With this clarified, it is now possible to begin discussion. The neural network in this implementation is constructed with 2 input neurons, i0 and i1, two hidden layer neurons, h0 and h1, and a single output neuron, O. After many iterations of back propagation, the connection between i0 and h0 is weighted at 8.37804906, between i0 and h1 is 0.95391195, between i1 and h0 is 8.37901047, and between i1 and h1 is 0.95391286. It is interesting that the weights between each neuron and hidden layer approaches the same value. This makes sense, as an exclusive OR function does not weigh either input separately, an input of [1,0] is not treated differently than that of [0,1], and our network's hidden layer represents that. Continuing the analysis of weights, the weight from h0 to the output is 36.19888036 and the weight from h1 to the output is -45.2600789. Due to the nature of the sigmoid activation function, this negative weight ensures that the output activation will only approach 1 when h0's activation is significantly higher than that of h1. Due to the weightings between the input and output layers, h0 will always have a high activation when there is at least one input value of 1. h1, however, will only return low enough to counteract its negative weighting if there is only one input value of 1; if there are two, the activation value for h1 will be high enough to bring down the activation of the output due to the aforementioned negative weighting. In the case of an input with two zeros, both h0 and h1 will have a low activation value, and thus O will also have a low activation value. As such, the weights at each layer ensure that the network produces a proper result for every input. For clarity on the structure of the network, a graphical network diagram is included in the file nn.png.

The results of running this network align well with what one would assume based upon the weights as discussed earlier. For clarity, my results were collected using the same network the weights were measured in. To begin, the results for the input [1,1] (a = 1 and b = 1) are as follows: the activation for h0 was 0.99999995, h1 was 0.87077453, and the output was 0.03870038. This is rounded down to 0, as the output value can only be 1 or 0, and printed to stdout. As mentioned before, this aligns well with the expectations given the behaviour of the network with regards to its weights. As h0 and h1 were close in value, the output had a low activation value. Continuing, the results for [0,1] were an activation of 0.99977041 for h0, 0.72190136 for h1, and 0.97117446 for the output, producing a final prediction of 1. The results for [1,0] consisted of an activation of 0.99977019 for h0, 0.72190117 for h1, and 0.97117446 for the output, producing a prediction of 1. Once again, this is consistent with our earlier findings. The activations for [1,0] and [0,1] are extremely similar, as neither i0 nor i1 are weighted significantly differently from each other, aligning with how XOR functions logically. Finally, the activations for [0,0] were a value of 0.5 for h0, 0.5 for h1, and 0.01065951 for the output producing a prediction of 0. 0.5 is the lowest value the sigmoid activation function can produce, which aligns with expectations as the sum of all inputs times the weights will be 0. 

In conclusion, through examining the weights of the neural network, much was learned about both the network's architecture and its expected behaviour. The weights allowed us to reasonably predict the activation behaviour at each level of the neural network, and the predictions were accurate when applied to the real data. In every case presented, a large difference between the activation value of h0 and h1 was necessary in order to output an activation around 1. This is the direct cause of our negative and positive weights between the output and h1 and h0 respectively. This large difference can only be caused when the two inputs are different, as the inputs' weights towards h0 and h1 were nearly indentical. Thus, the network accurately models the XOR function.